## ğŸ¤– LLM Data Parser

ä¸€å¥—æ•´åˆ **Ollama æœ¬åœ°æ¨è«–æœå‹™** çš„ Java APIï¼Œæä¾›å¿«é€Ÿå‘¼å«æœ¬åœ° LLM æ¨¡å‹ä¸¦è™•ç†è‡ªç„¶èªè¨€è³‡æ–™çš„èƒ½åŠ›ï¼Œè¨­è¨ˆç”¨æ–¼ç§Ÿå±‹è³‡æ–™çµæ§‹åŒ–ç­‰æ‡‰ç”¨ã€‚

å°ˆæ¡ˆåç¨±ï¼š**LLMDataParser**

---

### ğŸ“¦ åŠŸèƒ½ç°¡ä»‹

* ğŸ“¡ å‘¼å«æœ¬æ©Ÿ Ollama æ¨è«– APIï¼ˆæ”¯æ´æ¨¡å‹å¦‚ï¼š`llama3:8b`, `mistral`ï¼‰
* âš™ï¸ æ”¯æ´å¤šæ¨¡å‹åˆ‡æ›èˆ‡é…ç½®
* ğŸ§  æ­é…ç§Ÿå±‹ç¤¾åœ˜è²¼æ–‡æ ¼å¼é€²è¡Œ LLM å›æ‡‰è§£æ
* âœ… æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œæ˜“æ–¼æ•´åˆè‡³çˆ¬èŸ²æˆ–åˆ†æç³»çµ±ä¸­

---

### ğŸ“ å°ˆæ¡ˆçµæ§‹

```
LLMDataParser/
â”œâ”€â”€ .github/               # GitHub Actions æˆ– CI è¨­å®š
â”œâ”€â”€ .gradle/               # Gradle å¿«å–è³‡æ–™å¤¾
â”œâ”€â”€ .idea/                 # IntelliJ å°ˆæ¡ˆè³‡æ–™
â”œâ”€â”€ build/                 # ç·¨è­¯å¾Œçš„ç”¢å‡ºæª”æ¡ˆ
â”œâ”€â”€ gradle/                # Gradle Wrapper è¨­å®š
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â””â”€â”€ java/io/github/studentrentalsystem/
â”‚           â”œâ”€â”€ LLMAPIRules.java       # å›æ‡‰æ ¼å¼è¦å‰‡å®šç¾©
â”‚           â”œâ”€â”€ LLMClient.java         # å‘¼å« Ollama æ¨è«– API
â”‚           â”œâ”€â”€ LLMConfig.java         # è¨­å®šæª”èˆ‡æ¨¡å‹åƒæ•¸
â”‚           â””â”€â”€ ModelRegistry.java     # æ¨¡å‹ç®¡ç†
â”œâ”€â”€ .gitignore
â”œâ”€â”€ build.gradle.kts        # Gradle è¨­å®šæª”
â”œâ”€â”€ gradle.properties       # Gradle å±¬æ€§æª”
â”œâ”€â”€ gradlew / gradlew.bat   # Gradle åŸ·è¡Œè…³æœ¬
â”œâ”€â”€ settings.gradle.kts     # å°ˆæ¡ˆè¨­å®š
â”œâ”€â”€ README.md               # èªªæ˜æ–‡ä»¶ï¼ˆæœ¬æª”æ¡ˆï¼‰
```

---

### âš™ï¸ è¨­å®šæ–¹å¼

1. ç¢ºä¿å®‰è£ [Ollama](https://ollama.com/) ä¸¦åŸ·è¡Œï¼š

```bash
ollama run llama3:8b
```

2. è¨­å®šæ¨¡å‹èˆ‡æ¨è«–åƒæ•¸æ–¼ `LLMConfig.java`
```java
public LLMConfig(LLMMode mode, 
                 String serverAddress, 
                 int serverPort,
                 String modelType, 
                 boolean stream, 
                 BlockingQueue<LLMClient.StreamData> queue
) { ... }
```
- mode: CHAT, GENERATE, EMBEDDINGS
- serverAddress: your server address
- serverPort: your server port
- modelType: like "llama3:8b", "mistral", ...
- stream: used to specify whether to show LLM response instantly or not
- queue: used to receive response when stream mode is true


```java
LLMConfig config = new LLMConfig(
        LLMConfig.LLMMode.CHAT,
        "Your server address",
        "Your server port",
        "Your model type",
        false,
        null
);
```

---

### ğŸš€ ä½¿ç”¨æ–¹å¼

åœ¨ `LLMClient.java` ä¸­å¯ä½¿ç”¨ï¼š

```java
LLMClient client = new LLMClient(config);
String prompt = "è«‹å°‡ä»¥ä¸‹ç§Ÿå±‹è²¼æ–‡è½‰ç‚º JSON çµæ§‹...";
String result = client.callLocalModel(prompt);
```

æ­é… `LLMAPIRules` å°è¼¸å‡ºé€²è¡Œæ­£è¦åŒ–æˆ–éæ¿¾ã€‚

---

### ğŸ§  æ‡‰ç”¨å ´æ™¯

* ç§Ÿå±‹ç¤¾åœ˜è‡ªç„¶èªè¨€è²¼æ–‡çµæ§‹åŒ–ï¼ˆé…åˆ Facebook Crawler ä½¿ç”¨ï¼‰
* èŠå¤©è¨˜éŒ„è§£æ
* æ–‡å­—åˆ†é¡èˆ‡æ‘˜è¦

---

### ğŸ“„ æˆæ¬Š

MIT License

---

### âœ¨ è²¢ç»è€…

é–‹ç™¼è€…ï¼š[@hding4915](https://github.com/hding4915)
